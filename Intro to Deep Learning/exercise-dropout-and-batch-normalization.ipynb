{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Intro to Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/dropout-and-batch-normalization).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction #\n\nIn this exercise, you'll add dropout to the *Spotify* model from Exercise 4 and see how batch normalization can let you successfully train models on difficult datasets.\n\nRun the next cell to get started!","metadata":{}},{"cell_type":"code","source":"# Setup plotting\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')\n\n# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.deep_learning_intro.ex5 import *","metadata":{"execution":{"iopub.status.busy":"2022-06-01T13:51:26.942999Z","iopub.execute_input":"2022-06-01T13:51:26.943608Z","iopub.status.idle":"2022-06-01T13:51:27.028144Z","shell.execute_reply.started":"2022-06-01T13:51:26.943491Z","shell.execute_reply":"2022-06-01T13:51:27.027116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First load the *Spotify* dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import GroupShuffleSplit\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\nspotify = pd.read_csv('../input/dl-course-data/spotify.csv')\n\nX = spotify.copy().dropna()\ny = X.pop('track_popularity')\nartists = X['track_artist']\n\nfeatures_num = ['danceability', 'energy', 'key', 'loudness', 'mode',\n                'speechiness', 'acousticness', 'instrumentalness',\n                'liveness', 'valence', 'tempo', 'duration_ms']\nfeatures_cat = ['playlist_genre']\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), features_num),\n    (OneHotEncoder(), features_cat),\n)\n\ndef group_split(X, y, group, train_size=0.75):\n    splitter = GroupShuffleSplit(train_size=train_size)\n    train, test = next(splitter.split(X, y, groups=group))\n    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])\n\nX_train, X_valid, y_train, y_valid = group_split(X, y, artists)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\ny_train = y_train / 100\ny_valid = y_valid / 100\n\ninput_shape = [X_train.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T13:54:17.700186Z","iopub.execute_input":"2022-06-01T13:54:17.701456Z","iopub.status.idle":"2022-06-01T13:54:27.706859Z","shell.execute_reply.started":"2022-06-01T13:54:17.701372Z","shell.execute_reply":"2022-06-01T13:54:27.705614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1) Add Dropout to Spotify Model\n\nHere is the last model from Exercise 4. Add two dropout layers, one after the `Dense` layer with 128 units, and one after the `Dense` layer with 64 units. Set the dropout rate on both to `0.3`.","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE: Add two 30% dropout layers, one after 128 and one after 64\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1)\n])\n\n# Check your answer\nq_1.check()","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2022-06-01T14:01:25.157181Z","iopub.execute_input":"2022-06-01T14:01:25.157719Z","iopub.status.idle":"2022-06-01T14:01:25.249066Z","shell.execute_reply.started":"2022-06-01T14:01:25.157676Z","shell.execute_reply":"2022-06-01T14:01:25.248021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\nq_1.hint()\nq_1.solution()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T13:54:56.566674Z","iopub.execute_input":"2022-06-01T13:54:56.567154Z","iopub.status.idle":"2022-06-01T13:54:56.580059Z","shell.execute_reply.started":"2022-06-01T13:54:56.567121Z","shell.execute_reply":"2022-06-01T13:54:56.579067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now run this next cell to train the model see the effect of adding dropout.","metadata":{}},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='mae',\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=50,\n    verbose=0,\n)\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:01:34.351259Z","iopub.execute_input":"2022-06-01T14:01:34.351793Z","iopub.status.idle":"2022-06-01T14:01:39.42763Z","shell.execute_reply.started":"2022-06-01T14:01:34.351741Z","shell.execute_reply":"2022-06-01T14:01:39.426528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Evaluate Dropout\n\nRecall from Exercise 4 that this model tended to overfit the data around epoch 5. Did adding dropout seem to help prevent overfitting this time?","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this cell to receive credit!)\nq_2.check()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T13:56:35.959365Z","iopub.execute_input":"2022-06-01T13:56:35.959896Z","iopub.status.idle":"2022-06-01T13:56:35.968704Z","shell.execute_reply.started":"2022-06-01T13:56:35.959855Z","shell.execute_reply":"2022-06-01T13:56:35.96754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we'll switch topics to explore how batch normalization can fix problems in training.\n\nLoad the *Concrete* dataset. We won't do any standardization this time. This will make the effect of batch normalization much more apparent.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nconcrete = pd.read_csv('../input/dl-course-data/concrete.csv')\ndf = concrete.copy()\n\ndf_train = df.sample(frac=0.7, random_state=0)\ndf_valid = df.drop(df_train.index)\n\nX_train = df_train.drop('CompressiveStrength', axis=1)\nX_valid = df_valid.drop('CompressiveStrength', axis=1)\ny_train = df_train['CompressiveStrength']\ny_valid = df_valid['CompressiveStrength']\n\ninput_shape = [X_train.shape[1]]","metadata":{"execution":{"iopub.status.busy":"2022-06-01T13:59:01.637387Z","iopub.execute_input":"2022-06-01T13:59:01.637925Z","iopub.status.idle":"2022-06-01T13:59:01.659143Z","shell.execute_reply.started":"2022-06-01T13:59:01.637885Z","shell.execute_reply":"2022-06-01T13:59:01.657873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run the following cell to train the network on the unstandardized *Concrete* data.","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape=input_shape),\n    layers.Dense(512, activation='relu'),    \n    layers.Dense(512, activation='relu'),\n    layers.Dense(1),\n])\nmodel.compile(\n    optimizer='sgd', # SGD is more sensitive to differences of scale\n    loss='mae',\n    metrics=['mae'],\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=64,\n    epochs=100,\n    verbose=0,\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[0:, ['loss', 'val_loss']].plot()\nprint((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:00:18.500813Z","iopub.execute_input":"2022-06-01T14:00:18.501289Z","iopub.status.idle":"2022-06-01T14:00:32.373546Z","shell.execute_reply.started":"2022-06-01T14:00:18.501252Z","shell.execute_reply":"2022-06-01T14:00:32.372458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Did you end up with a blank graph? Trying to train this network on this dataset will usually fail. Even when it does converge (due to a lucky weight initialization), it tends to converge to a very large number.\n\n# 3) Add Batch Normalization Layers\n\nBatch normalization can help correct problems like this.\n\nAdd four `BatchNormalization` layers, one before each of the dense layers. (Remember to move the `input_shape` argument to the new first layer.)","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE: Add a BatchNormalization layer before each Dense layer\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(512, activation='relu', input_shape=input_shape),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n\n# Check your answer\nq_3.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2022-06-01T14:01:06.503686Z","iopub.execute_input":"2022-06-01T14:01:06.50485Z","iopub.status.idle":"2022-06-01T14:01:06.626668Z","shell.execute_reply.started":"2022-06-01T14:01:06.50479Z","shell.execute_reply":"2022-06-01T14:01:06.625294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\nq_3.hint()\nq_3.solution()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:00:37.350344Z","iopub.execute_input":"2022-06-01T14:00:37.350899Z","iopub.status.idle":"2022-06-01T14:00:37.364331Z","shell.execute_reply.started":"2022-06-01T14:00:37.350852Z","shell.execute_reply":"2022-06-01T14:00:37.363276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run the next cell to see if batch normalization will let us train the model.","metadata":{}},{"cell_type":"code","source":"model.compile(\n    optimizer='sgd',\n    loss='mae',\n    metrics=['mae'],\n)\nEPOCHS = 100\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=64,\n    epochs=EPOCHS,\n    verbose=0,\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[0:, ['loss', 'val_loss']].plot()\nprint((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:01:55.453726Z","iopub.execute_input":"2022-06-01T14:01:55.454303Z","iopub.status.idle":"2022-06-01T14:02:05.5734Z","shell.execute_reply.started":"2022-06-01T14:01:55.454259Z","shell.execute_reply":"2022-06-01T14:02:05.572344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Evaluate Batch Normalization\n\nDid adding batch normalization help?","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this cell to receive credit!)\nq_4.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2022-06-01T14:02:11.097663Z","iopub.execute_input":"2022-06-01T14:02:11.098388Z","iopub.status.idle":"2022-06-01T14:02:11.10752Z","shell.execute_reply.started":"2022-06-01T14:02:11.098336Z","shell.execute_reply":"2022-06-01T14:02:11.106487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keep Going #\n\n[**Create neural networks**](https://www.kaggle.com/ryanholbrook/binary-classification) for binary classification.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/intro-to-deep-learning/discussion) to chat with other learners.*","metadata":{}}]}